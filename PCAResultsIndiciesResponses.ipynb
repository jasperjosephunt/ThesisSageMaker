{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import itertools\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('ops').getOrCreate()\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from  pyspark.sql.functions import abs, pow\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "import boto3\n",
    "from io import BytesIO\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEPS\n",
    "\n",
    "1. Import Data\n",
    "2. Create umbrella directory\n",
    "3. Vectorize features\n",
    "\n",
    "      a. Apply PCA to features range 1 - 100\n",
    "            i. Create PCA directoy\n",
    "            ii. Apply Kmeans to each PCA range (2,101,1) , (105,2001,5)\n",
    "                \n",
    "                I. save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "-----------------------------------------------------------------------------\n",
    "--- Inptuts\n",
    "-----------------------------------------------------------------------------\n",
    "'''\n",
    "Analysis = 'PCA'\n",
    "Interp = 'NoInterp'\n",
    "dataSource = 'L6'\n",
    "Coor = 'Char'\n",
    "dataSetName = '{}dataScaled.csv'.format(dataSource)\n",
    "\n",
    "DRrange = range(100,101,1)\n",
    "clusterRange1 = range(2,4,1)\n",
    "clusterRange2 = range(105,111,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "-----------------------------------------------------------------------------\n",
    "--- Definitions\n",
    "-----------------------------------------------------------------------------\n",
    "'''\n",
    "Results = 'Results'\n",
    "Indicies = 'Indicies'\n",
    "Response = 'Response'\n",
    "Figures = 'Figures'\n",
    "\n",
    "# Configuring S3\n",
    "s3_bucket_name = 'jasper-ml-sagemaker'\n",
    "role = get_execution_role()\n",
    "\n",
    "client = boto3.client('s3')\n",
    "resource = boto3.resource('s3')\n",
    "my_bucket = resource.Bucket(s3_bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = 'Data/'\n",
    "\n",
    "# Dataset\n",
    "dataKey = dataSetName\n",
    "obj = client.get_object(Bucket=s3_bucket_name, Key=dataPath + dataKey)\n",
    "pd.read_csv(obj['Body'], index_col=None).to_csv(dataKey, index=False)\n",
    "dataSet = spark.read.csv(dataKey,inferSchema = True)\n",
    "\n",
    "# Response Data\n",
    "dataKey = '{}FixedResponse.npy'.format(dataSource)\n",
    "obj = client.get_object(Bucket=s3_bucket_name, Key=dataPath + dataKey)\n",
    "responseData = np.load(BytesIO(obj['Body'].read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing features\n",
    "assembler = VectorAssembler(inputCols= dataSet.columns,\n",
    "                           outputCol = 'features')\n",
    "vectorisedData = assembler.transform(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA 100 elements & 2 centres took 10.678924322128296 seconds\n",
      "Completed indicies for Dim = 100 & k = 2 in 0.11 s\n",
      "PCA 100 elements & 3 centres took 10.956296443939209 seconds\n",
      "Completed indicies for Dim = 100 & k = 3 in 0.13 s\n",
      "PCA 100 elements & 105 centres took 16.528581380844116 seconds\n",
      "Completed indicies for Dim = 100 & k = 105\n",
      "PCA 100 elements & 110 centres took 14.225117444992065 seconds\n",
      "Completed indicies for Dim = 100 & k = 110\n"
     ]
    }
   ],
   "source": [
    "# For PCA in range 2-101\n",
    "for j in DRrange:\n",
    "    \n",
    "    collectedIndicies = []\n",
    "    Dimensions = j\n",
    "    resultsPath = '{}/{}/{}/{}/{}/{}/'.format(dataSource,Coor,Interp,Analysis,Results,Dimensions)\n",
    "    \n",
    "    # Perform PCA on the data\n",
    "    pca = PCA(k=j, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "    PCAModel = pca.fit(vectorisedData)\n",
    "    \n",
    "    PCAData = PCAModel.transform(vectorisedData).select(\"pcaFeatures\")\n",
    "    to_array = F.udf(lambda v: v.toArray().tolist(), T.ArrayType(T.FloatType()))\n",
    "    profileArray = PCAData.withColumn('pcaFeatures', to_array('pcaFeatures')).toPandas()\n",
    "    profile = pd.DataFrame(profileArray.pcaFeatures.values.tolist(), index= profileArray.index)\n",
    "    \n",
    "    for i in clusterRange1:\n",
    "        #  Perform Kmeans on the data\n",
    "        timestart = time.time()\n",
    "        kmeans = KMeans(k=i, maxIter= 2000,featuresCol='pcaFeatures')\n",
    "        kMeansModel = kmeans.fit(PCAData)\n",
    "        predictions = kMeansModel.transform(PCAData).select('prediction').toPandas()\n",
    "\n",
    "        # save at each stage to reduce memory consumption\n",
    "        dataKey = '{}results.csv'.format(i)\n",
    "        predictions.to_csv(dataKey, index=False)\n",
    "        my_bucket.upload_file(dataKey,Key=resultsPath + dataKey)\n",
    "        os.remove(dataKey)\n",
    "        \n",
    "        print('PCA {} elements & {} centres took {} seconds'.format(j,i,time.time()-timestart))\n",
    "        \n",
    "        # Calculate characteristic indicies\n",
    "        timeStartIndicies = time.time()\n",
    "        profile2 = profile.copy()\n",
    "        profile2['labels'] = predictions.astype(np.float64)\n",
    "        charProfile = profile2.groupby(['labels']).mean().values\n",
    "        characteristicIndicies = []\n",
    "        for ii in range(i):\n",
    "            characteristicIndicies.append(np.argmin(np.linalg.norm(profile-charProfile[ii],axis=1)))\n",
    "            \n",
    "        collectedIndicies.append(np.flip(characteristicIndicies,axis=0))\n",
    "        print('Completed indicies for Dim = {} & k = {} in {:.2f} s'.format(Dimensions, i, time.time() - timeStartIndicies))\n",
    "\n",
    "    \n",
    "    for i in clusterRange2:\n",
    "        timestart = time.time()\n",
    "        kmeans = KMeans(k=i, maxIter= 2000,featuresCol='pcaFeatures')\n",
    "        kMeansModel = kmeans.fit(PCAData)\n",
    "        predictions = kMeansModel.transform(PCAData).select('prediction').toPandas()\n",
    "\n",
    "        \n",
    "        # save at each stage to reduce memory consumption\n",
    "        dataKey = '{}results.csv'.format(i)\n",
    "        predictions.to_csv(dataKey, index=False)\n",
    "        my_bucket.upload_file(dataKey,Key=resultsPath + dataKey)\n",
    "        os.remove(dataKey)\n",
    "        \n",
    "        print('PCA {} elements & {} centres took {} seconds'.format(j,i,time.time()-timestart))\n",
    "        \n",
    "        # Calculate characteristic indicies\n",
    "        timeStartIndicies = time.time()\n",
    "        profile2 = profile.copy()\n",
    "        profile2['labels'] = predictions.astype(np.float64)\n",
    "        charProfile = profile2.groupby(['labels']).mean().values\n",
    "        characteristicIndicies = []\n",
    "        for ii in range(i):\n",
    "            characteristicIndicies.append(np.argmin(np.linalg.norm(profile-charProfile[ii],axis=1)))\n",
    "            \n",
    "        collectedIndicies.append(np.flip(characteristicIndicies,axis=0))\n",
    "        print('Completed indicies for Dim = {} & k = {}'.format(Dimensions, i, time.time() - timeStartIndicies))\n",
    "    \n",
    "    #Uploading Char Indicies\n",
    "    indiciesPath = '{}/{}/{}/{}/{}/'.format(dataSource, Coor, Interp, Analysis, Indicies)\n",
    "    dataKey = '{}DimCharIndicies.npy'.format(Dimensions)\n",
    "    np.save(dataKey,collectedIndicies)\n",
    "    my_bucket.upload_file(dataKey,Key=indiciesPath + dataKey)\n",
    "    os.remove(dataKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_array = F.udf(lambda v: v.toArray().tolist(), T.ArrayType(T.FloatType()))\n",
    "profile = PCAData.withColumn('pcaFeatures', to_array('pcaFeatures')).toPandas()\n",
    "profile = pd.DataFrame(df.pcaFeatures.values.tolist(), index= df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = kMeansModel.transform(PCAData).select('prediction').toPandas()\n",
    "profile2 = profile.copy()\n",
    "profile2['labels'] = predictions.astype(np.float64)\n",
    "charProfile = profile2.groupby(['labels']).mean().values\n",
    "characteristicIndicies = []\n",
    "for ii in range(i):\n",
    "    characteristicIndicies.append(np.argmin(np.linalg.norm(pred_auto-charProfile[jj],axis=1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
