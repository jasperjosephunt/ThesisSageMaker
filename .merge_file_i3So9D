{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import keras.backend as K\n",
    "\n",
    "from time import time\n",
    "\n",
    "from keras import callbacks\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense, Input\n",
    "from keras.initializers import VarianceScaling\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "\n",
    "from scipy.misc import imread\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, normalized_mutual_info_score\n",
    "\n",
    "import boto3\n",
    "from io import BytesIO\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('ops').getOrCreate()\n",
    "from pyspark.ml.clustering import KMeans as KMeansSpark\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "-----------------------------------------------------------------------------\n",
    "--- Inputs\n",
    "-----------------------------------------------------------------------------\n",
    "'''\n",
    "Interp = 'NoInterp'\n",
    "dataSource = 'L6'\n",
    "Coor = 'Char'\n",
    "dataSetName = '{}dataScaled.csv'.format(dataSource)\n",
    "DRrange = range(1,2,1)\n",
    "clusterRange1 = range(2,4,1)\n",
    "clusterRange2 = range(105,106,5)\n",
    "\n",
    "# Initialise settings\n",
    "seed = 128\n",
    "init = 'glorot_uniform'\n",
    "pretrain_optimizer = 'adam'\n",
    "batch_size = 180\n",
    "maxiter = 2e4\n",
    "tol = 0.001\n",
    "update_interval = 50\n",
    "pretrain_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "-----------------------------------------------------------------------------\n",
    "--- Definitions\n",
    "-----------------------------------------------------------------------------\n",
    "'''\n",
    "Results = 'Results'\n",
    "Indicies = 'Indicies'\n",
    "Response = 'Response'\n",
    "Figures = 'Figures'\n",
    "\n",
    "# Configuring S3\n",
    "s3_bucket_name = 'jasper-ml-sagemaker'\n",
    "role = get_execution_role()\n",
    "\n",
    "client = boto3.client('s3')\n",
    "resource = boto3.resource('s3')\n",
    "my_bucket = resource.Bucket(s3_bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "-----------------------------------------------------------------------------\n",
    "--- Building AutoEncoder & Encoder\n",
    "-----------------------------------------------------------------------------\n",
    "'''\n",
    "def autoencoder(dims, act='relu', init='glorot_uniform'):\n",
    "\n",
    "    n_stacks = len(dims) - 1\n",
    "    # input\n",
    "    x = Input(shape=(dims[0],), name='input')\n",
    "    h = x\n",
    "\n",
    "    # internal layers in encoder\n",
    "    for i in range(n_stacks-1):\n",
    "        h = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(h)\n",
    "\n",
    "    # hidden layer\n",
    "    h = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(h)  # hidden layer, features are extracted from here\n",
    "\n",
    "    y = h\n",
    "    # internal layers in decoder\n",
    "    for i in range(n_stacks-1, 0, -1):\n",
    "        y = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(y)\n",
    "\n",
    "    # output\n",
    "    y = Dense(dims[0], kernel_initializer=init, name='decoder_0')(y)\n",
    "\n",
    "    return Model(inputs=x, outputs=y, name='AE'), Model(inputs=x, outputs=h, name='encoder')\n",
    "\n",
    "'''\n",
    "-----------------------------------------------------------------------------\n",
    "--- Building Clustering Layer\n",
    "-----------------------------------------------------------------------------\n",
    "'''\n",
    "\n",
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight(shape = (self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "'''\n",
    "-----------------------------------------------------------------------------\n",
    "--- Building Deep Embedded Clustering Class\n",
    "-----------------------------------------------------------------------------\n",
    "'''\n",
    "    \n",
    "class DEC(object):\n",
    "    def __init__(self,\n",
    "                 dims,\n",
    "                 autoencoder,\n",
    "                 encoder,\n",
    "                 n_clusters=10,\n",
    "                 alpha=1.0,\n",
    "                 init='glorot_uniform'):\n",
    "\n",
    "        super(DEC, self).__init__()\n",
    "\n",
    "        self.dims = dims\n",
    "        self.input_dim = dims[0]\n",
    "        self.n_stacks = len(self.dims) - 1\n",
    "\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.autoencoder = autoencoder\n",
    "        self.encoder= encoder\n",
    "        \n",
    "        # prepare DEC model\n",
    "        clustering_layer = ClusteringLayer(self.n_clusters, name='clustering')(self.encoder.output)\n",
    "        self.model = Model(inputs=self.encoder.input, outputs=clustering_layer)\n",
    "\n",
    "    def load_weights(self, weights):  # load weights of DEC model\n",
    "        self.model.load_weights(weights)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        return self.encoder.predict(x)\n",
    "\n",
    "    def predict(self, x):  # predict cluster labels using the output of clustering layer\n",
    "        q = self.model.predict(x, verbose=0)\n",
    "        return q.argmax(1)\n",
    "\n",
    "    @staticmethod\n",
    "    def target_distribution(q):\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return (weight.T / weight.sum(1)).T\n",
    "\n",
    "    def compile(self, optimizer='sgd', loss='kld'):\n",
    "        self.model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    def fit(self, x, y=None, maxiter=2e4, batch_size=256, tol=1e-3,\n",
    "            update_interval=140, save_dir='./results/temp'):\n",
    "\n",
    "        print('Update interval', update_interval)\n",
    "        save_interval = x.shape[0] / batch_size * 5  # 5 epochs\n",
    "        print('Save interval', save_interval)\n",
    "\n",
    "        # Step 1: initialize cluster centers using k-means\n",
    "        t1 = time()\n",
    "        print('Initializing cluster centers with k-means.')\n",
    "        \n",
    "        E_data = spark.createDataFrame(pd.DataFrame(self.encoder.predict(x)))\n",
    "        assembler = VectorAssembler(inputCols= E_data.columns,\n",
    "                                    outputCol = 'features')\n",
    "        vectorised_E_data = assembler.transform(E_data).select(\"features\")\n",
    "        kmeansSpark = KMeansSpark(k=self.n_clusters, maxIter= 300,featuresCol='features')\n",
    "        kMeansModelSpark = kmeansSpark.fit(vectorised_E_data)\n",
    "        \n",
    "        y_pred = kMeansModelSpark.transform(vectorised_E_data).select('prediction').toPandas()\n",
    "        y_pred_last = np.copy(y_pred)\n",
    "        self.model.get_layer(name='clustering').set_weights([np.array(kMeansModelSpark.clusterCenters())])\n",
    "\n",
    "        # Step 2: deep clustering\n",
    "        loss = 0\n",
    "        index = 0\n",
    "        index_array = np.arange(x.shape[0])\n",
    "        for ite in range(int(maxiter)):\n",
    "            if ite % update_interval == 0:\n",
    "                q = self.model.predict(x, verbose=0)\n",
    "                p = self.target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "                # evaluate the clustering performance\n",
    "                y_pred = q.argmax(1)\n",
    "                if y is not None:\n",
    "                    acc = np.round(metrics.acc(y, y_pred), 5)\n",
    "                    nmi = np.round(metrics.nmi(y, y_pred), 5)\n",
    "                    ari = np.round(metrics.ari(y, y_pred), 5)\n",
    "                    loss = np.round(loss, 5)\n",
    "                    logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, loss=loss)\n",
    "                    logwriter.writerow(logdict)\n",
    "                    print('Iter %d: acc = %.5f, nmi = %.5f, ari = %.5f' % (ite, acc, nmi, ari), ' ; loss=', loss)\n",
    "\n",
    "                # check stop criterion\n",
    "                delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
    "                y_pred_last = np.copy(y_pred)\n",
    "                if ite > 0 and delta_label < tol:\n",
    "                    print('delta_label ', delta_label, '< tol ', tol)\n",
    "                    print('Reached tolerance threshold. Stopping training.')\n",
    "                    #logfile.close()\n",
    "                    break\n",
    "\n",
    "            # train on batch\n",
    "            if index == 0:\n",
    "                 np.random.shuffle(index_array)\n",
    "            idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n",
    "            self.model.train_on_batch(x=x[idx], y=p[idx])\n",
    "            index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\n",
    "\n",
    "            # save intermediate model\n",
    "            #if ite % save_interval == 0:\n",
    "                #print('saving model to:', save_dir + '/DEC_model_' + str(ite) + '.h5')\n",
    "                #self.model.save_weights(save_dir + '/DEC_model_' + str(ite) + '.h5')\n",
    "\n",
    "            ite += 1\n",
    "\n",
    "        # save the trained model\n",
    "        \n",
    "        #print('saving model to:', save_dir + '/DEC_model_final.h5')\n",
    "        #self.model.save_weights(save_dir + '/DEC_model_final.h5')\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/50\n",
      "11359/11359 [==============================] - 2s 147us/step - loss: 0.8644\n",
      "Epoch 2/50\n",
      "11359/11359 [==============================] - 0s 29us/step - loss: 0.5480\n",
      "Epoch 3/50\n",
      "11359/11359 [==============================] - 0s 28us/step - loss: 0.4907\n",
      "Epoch 4/50\n",
      "11359/11359 [==============================] - 0s 28us/step - loss: 0.4900\n",
      "Epoch 5/50\n",
      "11359/11359 [==============================] - 0s 29us/step - loss: 0.4900\n",
      "Epoch 6/50\n",
      "11359/11359 [==============================] - 0s 28us/step - loss: 0.4900\n",
      "Epoch 7/50\n",
      "11359/11359 [==============================] - 0s 29us/step - loss: 0.4900\n",
      "Epoch 8/50\n",
      "11359/11359 [==============================] - 0s 29us/step - loss: 0.4899\n",
      "Epoch 9/50\n",
      "11359/11359 [==============================] - 0s 29us/step - loss: 0.4900\n",
      "Epoch 10/50\n",
      "11359/11359 [==============================] - 0s 28us/step - loss: 0.4900\n",
      "Epoch 11/50\n",
      "11359/11359 [==============================] - 0s 28us/step - loss: 0.4900\n",
      "Epoch 12/50\n",
      "11359/11359 [==============================] - 0s 28us/step - loss: 0.4900\n",
      "Epoch 13/50\n",
      "11359/11359 [==============================] - 0s 29us/step - loss: 0.4901\n",
      "Epoch 14/50\n",
      "11359/11359 [==============================] - 0s 29us/step - loss: 0.4901\n",
      "Epoch 15/50\n",
      "11359/11359 [==============================] - 0s 29us/step - loss: 0.4900\n",
      "Epoch 16/50\n",
      "11359/11359 [==============================] - 0s 28us/step - loss: 0.4900\n",
      "Epoch 17/50\n",
      "11359/11359 [==============================] - 0s 28us/step - loss: 0.4901\n",
      "Epoch 18/50\n",
      "11359/11359 [==============================] - 0s 29us/step - loss: 0.4899\n",
      "Epoch 19/50\n",
      "11359/11359 [==============================] - 0s 28us/step - loss: 0.4898\n",
      "Epoch 20/50\n",
      "11359/11359 [==============================] - 0s 28us/step - loss: 0.4900\n",
      "Epoch 21/50\n",
      "11359/11359 [==============================] - 0s 29us/step - loss: 0.4899\n",
      "Epoch 22/50\n",
      "11359/11359 [==============================] - 0s 29us/step - loss: 0.4900\n",
      "Epoch 23/50\n",
      "11359/11359 [==============================] - 0s 29us/step - loss: 0.4900\n",
      "Epoch 24/50\n",
      "11359/11359 [==============================] - 0s 29us/step - loss: 0.4900\n",
      "Epoch 25/50\n",
      "11359/11359 [==============================] - 0s 29us/step - loss: 0.4901\n",
      "Epoch 26/50\n",
      "11359/11359 [==============================] - 0s 30us/step - loss: 0.4901\n",
      "Epoch 27/50\n",
      "11359/11359 [==============================] - 0s 30us/step - loss: 0.4900\n",
      "Epoch 28/50\n",
      "11359/11359 [==============================] - 0s 30us/step - loss: 0.4900\n",
      "Epoch 29/50\n",
      "11359/11359 [==============================] - 0s 30us/step - loss: 0.4898\n",
      "Epoch 30/50\n",
      "11359/11359 [==============================] - 0s 30us/step - loss: 0.4901\n",
      "Epoch 31/50\n",
      "11359/11359 [==============================] - 0s 30us/step - loss: 0.4900\n",
      "Epoch 32/50\n",
      "11359/11359 [==============================] - 0s 30us/step - loss: 0.4900\n",
      "Epoch 33/50\n",
      "11359/11359 [==============================] - 0s 32us/step - loss: 0.4899\n",
      "Epoch 34/50\n",
      "11359/11359 [==============================] - 0s 30us/step - loss: 0.4903\n",
      "Epoch 35/50\n",
      "11359/11359 [==============================] - 0s 29us/step - loss: 0.4901\n",
      "Epoch 36/50\n",
      "11359/11359 [==============================] - 0s 30us/step - loss: 0.4900\n",
      "Epoch 37/50\n",
      "11359/11359 [==============================] - 0s 29us/step - loss: 0.4901\n",
      "Epoch 38/50\n",
      "11359/11359 [==============================] - 0s 30us/step - loss: 0.4901\n",
      "Epoch 39/50\n",
      "11359/11359 [==============================] - 0s 29us/step - loss: 0.4900\n",
      "Epoch 40/50\n",
      "11359/11359 [==============================] - 0s 29us/step - loss: 0.4900\n",
      "Epoch 41/50\n",
      "11359/11359 [==============================] - 0s 29us/step - loss: 0.4900\n",
      "Epoch 42/50\n",
      "11359/11359 [==============================] - 0s 30us/step - loss: 0.4899\n",
      "Epoch 43/50\n",
      "11359/11359 [==============================] - 0s 29us/step - loss: 0.4899\n",
      "Epoch 44/50\n",
      "11359/11359 [==============================] - 0s 29us/step - loss: 0.4900\n",
      "Epoch 45/50\n",
      "11359/11359 [==============================] - 0s 29us/step - loss: 0.4899\n",
      "Epoch 46/50\n",
      "11359/11359 [==============================] - 0s 29us/step - loss: 0.4901\n",
      "Epoch 47/50\n",
      "11359/11359 [==============================] - 0s 28us/step - loss: 0.4899\n",
      "Epoch 48/50\n",
      "11359/11359 [==============================] - 0s 28us/step - loss: 0.4900\n",
      "Epoch 49/50\n",
      "11359/11359 [==============================] - 0s 30us/step - loss: 0.4899\n",
      "Epoch 50/50\n",
      "11359/11359 [==============================] - 0s 29us/step - loss: 0.4901\n",
      "Pretraining time:  19.307438850402832\n",
      "Straight AE 1 elements & 2 centres took 1.8601977825164795 seconds\n",
      "Straight AE 1 elements & 3 centres took 0.2849385738372803 seconds\n",
      "Straight AE 1 elements & 105 centres took 6.848684072494507 seconds\n",
      "Update interval 140\n",
      "Save interval 221.85546875\n",
      "Initializing cluster centers with k-means.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "delta_label  0.0007042873492384893 < tol  0.001\n",
      "Reached tolerance threshold. Stopping training.\n",
      "DEC 1 elements & 2 centres took 11.190194845199585 seconds\n",
      "Update interval 140\n",
      "Save interval 221.85546875\n",
      "Initializing cluster centers with k-means.\n",
      "delta_label  0.0009683951052029228 < tol  0.001\n",
      "Reached tolerance threshold. Stopping training.\n",
      "DEC 1 elements & 3 centres took 49.32115459442139 seconds\n",
      "Update interval 140\n",
      "Save interval 221.85546875\n",
      "Initializing cluster centers with k-means.\n",
      "DEC 1 elements & 105 centres took 111.62504363059998 seconds\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "-----------------------------------------------------------------------------\n",
    "--- Loading Data\n",
    "-----------------------------------------------------------------------------\n",
    "'''\n",
    "dataPath = 'Data/'\n",
    "dataKey = dataSetName\n",
    "obj = client.get_object(Bucket=s3_bucket_name, Key=dataPath + dataKey)\n",
    "dataset = pd.read_csv(obj['Body'],header = None, index_col=None)\n",
    "\n",
    "'''\n",
    "-----------------------------------------------------------------------------\n",
    "--- Performing Analysis\n",
    "-----------------------------------------------------------------------------\n",
    "'''\n",
    "for i in DRrange:\n",
    "    \n",
    "    \n",
    "    Dimensions = i # To be checked\n",
    "    dims = [dataset.shape[-1],i]\n",
    "    \n",
    "    \n",
    "    # Build encoders\n",
    "    auto_encoder, encoder = autoencoder(dims, act='relu', init='glorot_uniform')\n",
    "    auto_encoder.compile(optimizer=pretrain_optimizer, loss='mse')\n",
    "    t0 = time()\n",
    "    train_history = auto_encoder.fit(dataset.values, dataset.values, batch_size=batch_size, epochs=pretrain_epochs)\n",
    "    print('Pretraining time: ', time() - t0)\n",
    "    \n",
    "    # Apply Encoder\n",
    "    pred_auto = encoder.predict(dataset.values).astype(np.float64)\n",
    "    \n",
    "    # Prepare data for Spark KMeans\n",
    "    AE_data = spark.createDataFrame(pd.DataFrame(pred_auto))\n",
    "    assembler = VectorAssembler(inputCols= AE_data.columns,\n",
    "                                outputCol = 'features')\n",
    "    vectorised_AE_data = assembler.transform(AE_data).select(\"features\")\n",
    "    \n",
    "    # Define path for saving in s3\n",
    "    Analysis = 'AE_Simple'\n",
    "    resultsPath = '{}/{}/{}/{}/{}/{}/'.format(dataSource,Coor,Interp,Analysis,Results,Dimensions)\n",
    "    collectedIndicies = [] \n",
    "    for j in clusterRange1:\n",
    "        \n",
    "        #  Perform Kmeans on the data\n",
    "        timestart = time()\n",
    "        kmeans = KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
    "                        n_clusters=j, n_init=20, n_jobs=-1, precompute_distances='auto',\n",
    "                        random_state=seed, tol=0.0001, verbose=0)\n",
    "        predictions = kmeans.fit_predict(pred_auto)\n",
    "        \n",
    "        # Save and upload to s3\n",
    "        dataKey = '{}results.csv'.format(j)\n",
    "        np.savetxt(dataKey,predictions,delimiter=',')\n",
    "        my_bucket.upload_file(dataKey,Key=resultsPath + dataKey)\n",
    "        os.remove(dataKey)\n",
    "        \n",
    "        # Computing Char Indicies\n",
    "        profile2 = pd.DataFrame(pred_auto)\n",
    "        profile2['labels'] = predictions.astype(np.float64)\n",
    "        charProfile = profile2.groupby(['labels']).mean().values\n",
    "        characteristicIndicies = []\n",
    "        for jj in range(j):\n",
    "            characteristicIndicies.append(np.argmin(np.linalg.norm(pred_auto-charProfile[jj],axis=1)))\n",
    "        \n",
    "        collectedIndicies.append(np.flip(characteristicIndicies,axis=0))\n",
    "        \n",
    "        print('Straight AE {} elements & {} centres took {} seconds'.format(i,j,time()-timestart))\n",
    "    \n",
    "    for j in clusterRange2:\n",
    "        \n",
    "        #  Perform Kmeans on the data\n",
    "        timestart = time()\n",
    "        kmeansSpark = KMeansSpark(k=j, maxIter= 300,featuresCol='features',seed = seed)\n",
    "        kMeansModelSpark = kmeansSpark.fit(vectorised_AE_data)\n",
    "        predictions = kMeansModelSpark.transform(vectorised_AE_data).select('prediction').toPandas()\n",
    "        \n",
    "        # save at each stage to reduce memory consumption\n",
    "        dataKey = '{}results.csv'.format(j)\n",
    "        predictions.to_csv(dataKey,index=False)\n",
    "        my_bucket.upload_file(dataKey,Key=resultsPath + dataKey)\n",
    "        os.remove(dataKey)\n",
    "        \n",
    "        # Computing Char Indicies\n",
    "        profile2 = pd.DataFrame(pred_auto)\n",
    "        profile2['labels'] = predictions.astype(np.float64)\n",
    "        charProfile = profile2.groupby(['labels']).mean().values\n",
    "        characteristicIndicies = []\n",
    "        for jj in range(j):\n",
    "            characteristicIndicies.append(np.argmin(np.linalg.norm(pred_auto-charProfile[jj],axis=1)))\n",
    "        \n",
    "        collectedIndicies.append(np.flip(characteristicIndicies,axis=0))\n",
    "        \n",
    "        print('Straight AE {} elements & {} centres took {} seconds'.format(i,j,time()-timestart))\n",
    "      \n",
    "    #Uploading Char Indicies\n",
    "    indiciesPath = '{}/{}/{}/{}/{}/'.format(dataSource, Coor, Interp, Analysis, Indicies)\n",
    "    dataKey = '{}DimCharIndicies.npy'.format(Dimensions)\n",
    "    np.save(dataKey,collectedIndicies)\n",
    "    my_bucket.upload_file(dataKey,Key=indiciesPath + dataKey)\n",
    "    os.remove(dataKey)\n",
    "    \n",
    "    Analysis = 'DEC_Simple'\n",
    "    resultsPath = '{}/{}/{}/{}/{}/{}/'.format(dataSource,Coor,Interp,Analysis,Results,Dimensions)\n",
    "    collectedIndicies = []   \n",
    "    for j in clusterRange1:\n",
    "        \n",
    "        timestart = time()\n",
    "        dec = DEC(dims=dims,n_clusters=j ,init = init,autoencoder = auto_encoder,encoder = encoder)\n",
    "        dec.compile(optimizer=SGD(0.01, 0.9), loss='kld')\n",
    "        \n",
    "        # Save and upload to s3\n",
    "        dataKey = '{}results.csv'.format(j)\n",
    "        np.savetxt(dataKey,dec.fit(x=dataset.values.astype(np.float64), y=None),delimiter=',')\n",
    "        my_bucket.upload_file(dataKey,Key=resultsPath + dataKey)\n",
    "        os.remove(dataKey)\n",
    "        \n",
    "        profile1 = pd.DataFrame(dec.model.predict(dataset.values.astype(np.float64)))\n",
    "        profile2 = profile1.copy()\n",
    "        profile2['labels'] = dec.predict(dataset.values.astype(np.float64))\n",
    "        charProfile = profile2.groupby(['labels']).mean().values\n",
    "        characteristicIndicies = []\n",
    "        for jj in range(j):\n",
    "            characteristicIndicies.append(np.argmin(np.linalg.norm(profile1.values-charProfile[jj],axis=1)))\n",
    "        \n",
    "        collectedIndicies.append(np.flip(characteristicIndicies,axis=0))\n",
    "        \n",
    "        print('DEC {} elements & {} centres took {} seconds'.format(i,j,time()-timestart))\n",
    "      \n",
    "    for j in clusterRange2:\n",
    "        \n",
    "        timestart = time()\n",
    "        dec = DEC(dims=dims,n_clusters=j ,init = init,autoencoder = auto_encoder,encoder = encoder)\n",
    "        dec.compile(optimizer=SGD(0.01, 0.9), loss='kld')\n",
    "        \n",
    "        # Save and upload to s3\n",
    "        dataKey = '{}results.csv'.format(j)\n",
    "        np.savetxt(dataKey,dec.fit(x=dataset.values.astype(np.float64), y=None),delimiter=',')\n",
    "        my_bucket.upload_file(dataKey,Key=resultsPath + dataKey)\n",
    "        os.remove(dataKey)\n",
    "        \n",
    "        profile1 = pd.DataFrame(dec.model.predict(dataset.values.astype(np.float64)))\n",
    "        profile2 = profile1.copy()\n",
    "        profile2['labels'] = dec.predict(dataset.values.astype(np.float64))\n",
    "        charProfile = profile2.groupby(['labels']).mean().values\n",
    "        characteristicIndicies = []\n",
    "        for jj in range(j):\n",
    "            characteristicIndicies.append(np.argmin(np.linalg.norm(profile1.values-charProfile[jj],axis=1)))\n",
    "        collectedIndicies.append(np.flip(characteristicIndicies,axis=0))\n",
    "        \n",
    "        print('DEC {} elements & {} centres took {} seconds'.format(i,j,time()-timestart))\n",
    "    \n",
    "    \n",
    "    #Uploading Char Indicies\n",
    "    indiciesPath = '{}/{}/{}/{}/{}/'.format(dataSource, Coor, Interp, Analysis, Indicies)\n",
    "    dataKey = '{}DimCharIndicies.npy'.format(Dimensions)\n",
    "    np.save(dataKey,collectedIndicies)\n",
    "    my_bucket.upload_file(dataKey,Key=indiciesPath + dataKey)\n",
    "    os.remove(dataKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "profile1 = pd.DataFrame(dec.model.predict(dataset.values.astype(np.float64)))\n",
    "profile2 = profile1.copy()\n",
    "profile2['labels'] = dec.predict(dataset.values.astype(np.float64))\n",
    "charProfile = profile2.groupby(['labels']).mean().values\n",
    "characteristicIndicies = []\n",
    "for jj in range(i):\n",
    "    characteristicIndicies.append(np.argmin(np.linalg.norm(profile1.values-charProfile[jj],axis=1)))\n",
    "collectedIndicies.append(np.flip(characteristicIndicies,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([4167]), array([4167])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectedIndicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
